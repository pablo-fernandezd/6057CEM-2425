{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "The field of machine learning has seen significant advancements in recent years, with neural networks becoming increasingly popular for solving complex classification problems. In this project, we aim to develop and evaluate neural network models for a real-world dataset, specifically the Iris Dataset.\n",
    "\n",
    "## Dataset Details\n",
    "\n",
    "The Iris Dataset, available from the UCI Machine Learning Repository, is a classic dataset in the field of machine learning. It consists of:\n",
    "\n",
    "- 150 samples\n",
    "- 4 features: sepal length, sepal width, petal length, and petal width\n",
    "- 3 target classes: Setosa, Versicolour, and Virginica\n",
    "\n",
    "This dataset is particularly suitable for classification tasks and serves as an excellent starting point for understanding neural network performance on real-world data.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The main objectives of this project are:\n",
    "\n",
    "1. To perform a comprehensive Exploratory Data Analysis (EDA) on the Iris Dataset\n",
    "2. To design and implement two different neural network architectures\n",
    "3. To train these models using appropriate learning techniques\n",
    "4. To evaluate and compare the performance of the models using various metrics\n",
    "\n",
    "## Approach\n",
    "\n",
    "Our approach to this project will be structured as follows:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**: We will begin by preprocessing the data and conducting a thorough EDA. This will include data visualization, feature scaling, and splitting the dataset into training, validation, and test sets.\n",
    "\n",
    "2. **Neural Network Design**: We will design two different neural network architectures, such as a Feed-Forward Neural Network and a Convolutional Neural Network. The choice of architectures will be justified based on the characteristics of the Iris Dataset and the classification problem at hand.\n",
    "\n",
    "3. **Model Training**: The models will be trained using Keras, employing appropriate learning techniques such as backpropagation and the Adam optimizer.\n",
    "\n",
    "4. **Evaluation**: We will evaluate the models using various metrics including accuracy, precision, recall, F1-score, and confusion matrices. This comprehensive evaluation will allow us to compare the performance of our models and discuss their strengths and limitations.\n",
    "\n",
    "Through this project, we aim to gain practical experience in applying neural networks to real-world data and to develop a deeper understanding of the challenges and considerations involved in model development and evaluation."
   ],
   "id": "f11742e1e8e8bd32"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "# Training frameworks\n",
    "from pipeline.training_framework import ModelTrainer\n",
    "# Utilities\n",
    "from utils.data_processing import IrisDataEngineer\n",
    "from utils.evaluation_metrics import AdvancedAnalytics\n",
    "from utils.eda_module import run_eda"
   ],
   "id": "3386288a4951aae7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    # ---------------- Set Reproducibility ----------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "id": "72a32ecb18346936",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Dataset Introduction\n",
    "\n",
    "The Iris dataset, introduced by British statistician and biologist Ronald Fisher in 1936, represents one of the earliest examples of multivariate analysis[15]. It consists of 150 samples from three species of Iris flowers (Setosa, Versicolour, and Virginica), with 50 samples per species. Each sample includes four features:\n",
    "\n",
    "- Sepal Length (cm)\n",
    "- Sepal Width (cm)\n",
    "- Petal Length (cm)\n",
    "- Petal Width (cm)\n",
    "\n",
    "### Historical and Scientific Significance\n",
    "\n",
    "The Iris dataset has become a cornerstone in machine learning and pattern recognition for several reasons:\n",
    "\n",
    "- **Benchmark Status**: It serves as a standard benchmark for classification algorithms due to its clean structure and moderate complexity\n",
    "- **Educational Value**: The dataset provides an ideal starting point for teaching classification techniques\n",
    "- **Feature Relationships**: The three classes are linearly separable in some dimensions but not in others, creating an interesting classification challenge\n",
    "- **Real-world Representation**: It represents actual biological measurements, connecting machine learning to scientific applications\n",
    "\n",
    "In this project, we'll examine whether advanced neural network architectures (Residual and Attention-based networks) provide any advantages over simpler models for this classic dataset.\n",
    "\n",
    "\n",
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "We load and process the Iris dataset using a custom `IrisDataEngineer` class. The class handles:\n",
    "- Loading the dataset from `sklearn.datasets`.\n",
    "- Encoding target labels numerically.\n",
    "- Splitting the data into training (60%), validation (20%), and test (20%) sets using stratified sampling.\n",
    "- Applying `StandardScaler` to normalize features to zero mean and unit variance.\n",
    "\n",
    "This preprocessing pipeline is crucial for stabilizing and accelerating neural network training.\n",
    "\n"
   ],
   "id": "262975ee1884ed4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Load and Preprocess Data ----------------\n",
    "print(\"\\nüöÄ Initializing Data Pipeline...\")\n",
    "processor = IrisDataEngineer()\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = processor.process()\n",
    "class_names = load_iris().target_names.tolist()"
   ],
   "id": "76fcd515124dc5ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Our EDA process leverages visualization techniques to understand the dataset's characteristics before model development. The `eda_module.py` provides comprehensive visualization capabilities to extract meaningful insights from the Iris dataset.\n",
    "\n",
    "### 4.1 Data Distribution Analysis\n",
    "\n",
    "The histograms below reveal the distribution of each feature across the three Iris species:\n",
    "\n",
    "- **Sepal Dimensions**: While there is some overlap in sepal measurements across species, Setosa typically has shorter but wider sepals compared to the other species\n",
    "- **Petal Dimensions**: Petal measurements show clearer separation between species, with Setosa having distinctly smaller petals, making these features particularly valuable for classification\n",
    "\n",
    "### 4.2 Feature Correlation and Relationships\n",
    "\n",
    "The correlation matrix and pairwise scatter plots reveal:\n",
    "\n",
    "- **Strong Positive Correlation**: Petal length and petal width show strong positive correlation (r > 0.9), suggesting these features carry similar information\n",
    "- **Moderate Correlation**: Sepal length correlates moderately with petal dimensions\n",
    "- **Species Clustering**: The scatter plots demonstrate that Setosa forms a distinct cluster, while Versicolor and Virginica show some overlap\n",
    "\n",
    "### 4.3 Dimensionality Reduction Visualization\n",
    "\n",
    "PCA and t-SNE visualizations reduce the 4-dimensional feature space to 2 dimensions:\n",
    "\n",
    "- **PCA**: The first two principal components explain approximately 95% of the variance, with clear separation between Setosa and the other species\n",
    "- **t-SNE**: This non-linear technique further enhances visualization of the cluster separation, particularly between Versicolor and Virginica\n",
    "\n",
    "These insights inform our modeling approach by highlighting the relative importance of features and the inherent separability of the classes.\n"
   ],
   "id": "dd5337b7f18ebfc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Exploratory Data Analysis (EDA) ----------------\n",
    "# Load the full structured dataset before any train/validation/test split\n",
    "df_raw = processor._load_structured_data()\n",
    "X_full = df_raw.drop('species', axis=1)\n",
    "y_full = df_raw['species']\n",
    "\n",
    "# This will generate summary statistics and all relevant visualizations for the full dataset\n",
    "run_eda(X_full, y_full)\n"
   ],
   "id": "cde944b4ac594b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "   ## 5. Model Trainers Instantiation\n",
    "To streamline our experiments, we instantiate two `ModelTrainer` objects ‚Äì one for the *residual* network and one for the *attention-based* network. The `ModelTrainer` class abstracts away the model-building and training processes for each architecture, allowing us to initialize a model by specifying its type (`'residual'` or `'attention'`). This design promotes modularity and ensures a consistent training pipeline for both models. By providing the input shape (4 features for Iris) and number of classes (3 species), each `ModelTrainer` knows how to construct the appropriate neural network with the correct input and output dimensions.\n"
   ],
   "id": "37fe933c2f3e61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Instantiate Model Trainers ----------------\n",
    "# We use a unified ModelTrainer class for both residual and attention models\n",
    "residual_trainer = ModelTrainer('residual', input_shape=X_train.shape[1:], num_classes=len(class_names))\n",
    "attention_trainer = ModelTrainer('attention', input_shape=X_train.shape[1:], num_classes=len(class_names))\n",
    "print(\"‚úÖ ModelTrainer class ready for 'residual' and 'attention' models.\")"
   ],
   "id": "3727a19d852dba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Residual Network Training and Initial Evaluation\n",
    "Here we construct and train the Residual Neural Network. This architecture includes skip connections to improve gradient flow and support deeper learning. After model creation, we train it using early stopping to avoid overfitting. The evaluation step includes accuracy, F1-score, and ROC-AUC, which establish a baseline performance for the residual model.\n"
   ],
   "id": "3336192bfa73c89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Initialize Evaluation Module ----------------\n",
    "analyst = AdvancedAnalytics(class_names)\n",
    "\n",
    "# ---------------- Residual Model (Manual) ----------------\n",
    "print(\"\\nüß† Building and Training Residual Model...\")\n",
    "residual_model = residual_trainer.create_model()\n",
    "residual_model.summary()\n",
    "residual_history = residual_trainer.train(residual_model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nüîç Evaluating Residual Model...\")\n",
    "analyst.full_analysis(residual_model, X_test, y_test)   "
   ],
   "id": "2595e782bc7ef668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Attention Network Training and Initial Evaluation\n",
    "We now train the Attention-Based Neural Network, designed to focus on the most informative features. After creating the model and reviewing the architecture, we train it under the same settings as the residual network. The evaluation metrics on the test set (including precision and ROC-AUC) help us assess its performance and establish a baseline for comparison.\n"
   ],
   "id": "8062aa2c2b9b85ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Attention Model (Manual) ----------------\n",
    "print(\"\\nüß† Building and Training Attention Model...\")\n",
    "attention_model = attention_trainer.create_model()\n",
    "attention_model.summary()\n",
    "attention_history = attention_trainer.train(attention_model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nüîç Evaluating Attention Model...\")\n",
    "analyst.full_analysis(attention_model, X_test, y_test)"
   ],
   "id": "484c4ce33257f28f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "Next, we use automated tuning (e.g., Keras Tuner) to optimize key hyperparameters like the number of units and learning rate. \n",
    "For each architecture, the tuner searches the space for the best validation performance. \n",
    "This step is important to ensure each model is trained under ideal conditions and to test how much tuning improves generalization.\n"
   ],
   "id": "8b217c9920280cea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Hyperparameter Tuning ----------------\n",
    "print(\"\\nüîß Tuning Residual Model...\")\n",
    "best_residual, best_residual_hp = residual_trainer.tune_model(X_train, y_train, X_val, y_val)\n",
    "print(\"Best Residual Hyperparameters:\", best_residual_hp.values)\n",
    "\n",
    "print(\"\\nüîß Tuning Attention Model...\")\n",
    "best_attention, best_attention_hp = attention_trainer.tune_model(X_train, y_train, X_val, y_val)\n",
    "print(\"Best Attention Hyperparameters:\", best_attention_hp.values)\n"
   ],
   "id": "a88d742620b18af7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Retraining Models with Best Hyperparameters\n",
    "With the best hyperparameters selected, we retrain each model from scratch to obtain the final tuned versions. This allows us to compare them fairly with the earlier manually-tuned versions and analyze how tuning affects learning dynamics and final performance.\n"
   ],
   "id": "5448b843dd630b0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Retraining Models with Best Hyperparameters ----------------\n",
    "print(\"\\nüî• Final Training with Tuned Residual Model...\")\n",
    "residual_history_tuned = residual_trainer.train(best_residual, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nüî• Final Training with Tuned Attention Model...\")\n",
    "attention_history_tuned = attention_trainer.train(best_attention, X_train, y_train, X_val, y_val)"
   ],
   "id": "3851011c709f2513",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Saving Models to Disk\n",
    "Once models are retrained with optimal parameters, we save them to disk in `.keras` format. This is a best practice for reproducibility and enables future re-use or deployment of the models without retraining.\n"
   ],
   "id": "1e18dc58fc489f19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Save Models to Disk ----------------\n",
    "residual_trainer.save_model(best_residual)\n",
    "attention_trainer.save_model(best_attention)"
   ],
   "id": "f62f3791ed84bdab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Reloading Models for Evaluation\n",
    "To confirm our saved models are usable, we reload them into memory. This mimics deployment or reuse in a different session, and ensures that the saved weights and architecture restore correctly. These reloaded models are now ready for final evaluation.\n"
   ],
   "id": "314482a25ad7ca6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Reload Models for Evaluation ----------------\n",
    "loaded_residual = residual_trainer.load_model()\n",
    "loaded_attention = attention_trainer.load_model()"
   ],
   "id": "8e5b9ffdc9ea2052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 12. Final Evaluation of Tuned Models\n",
    "We now perform a comprehensive evaluation of the reloaded models on the test set. Key metrics like precision, recall, F1-score, and ROC-AUC are examined. Here, the attention model clearly outperforms the residual network, especially in recall for the more ambiguous classes.\n"
   ],
   "id": "f53d72ef1954f3ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Final Evaluation of Tuned Models ----------------\n",
    "print(\"\\nüîç Evaluating Tuned Residual Model...\")\n",
    "analyst.full_analysis(loaded_residual, X_test, y_test)\n",
    "\n",
    "print(\"\\nüîç Evaluating Tuned Attention Model...\")\n",
    "analyst.full_analysis(loaded_attention, X_test, y_test)"
   ],
   "id": "8bbdd1147a6beaa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13. Training History Comparison Functions\n",
    "To visualize model performance over time, we define a utility function to compare training histories. This function helps us understand how validation accuracy and loss evolve across epochs and compare learning dynamics between models.\n"
   ],
   "id": "c703d0a896c5c8ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------- Comparison Plot Functions ----------------\n",
    "def compare_models(histories, labels, title_suffix):\n",
    "    \"\"\"Plot side-by-side comparison of validation accuracy and loss for multiple models.\"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    # Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for history, label in zip(histories, labels):\n",
    "        if 'val_sparse_categorical_accuracy' in history.history:\n",
    "            plt.plot(history.history['val_sparse_categorical_accuracy'], label=label)\n",
    "    plt.title(f'Validation Accuracy Comparison - {title_suffix}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    # Validation Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for history, label in zip(histories, labels):\n",
    "        plt.plot(history.history['val_loss'], label=label)\n",
    "    plt.title(f'Validation Loss Comparison - {title_suffix}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "24cc089abf6e64ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## 14. Individual Training History Plotting\n",
    "This function plots training and validation accuracy and loss curves for a single model. It's helpful for spotting overfitting or underfitting by visualizing how the model behaves across training epochs. We'll use this to analyze each model's performance.\n"
   ],
   "id": "6e105df4f07a6131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Individual Training History Plot Function ---\n",
    "def plot_individual(history, title):\n",
    "    \"\"\"Plot accuracy and loss curves for a single model training history.\"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.suptitle(title)\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "c7961764125707c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 15. Individual Model Training Curves\n",
    "We now visualize each model's training history using the `plot_individual` function. \n",
    "These plots reveal key insights into how each model learned over time, and where overfitting or efficient learning occurred.\n"
   ],
   "id": "38adc43b8df61653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Plot Individual Histories ----------------\n",
    "# Each plot shows accuracy and loss for one model during training\n",
    "plot_individual(residual_history, 'Residual Model (Manual Parameters)')\n",
    "plot_individual(attention_history, 'Attention Model (Manual Parameters)')\n",
    "plot_individual(residual_history_tuned, 'Residual Model (Tuned with Hyperparameters)')\n",
    "plot_individual(attention_history_tuned, 'Attention Model (Tuned with Hyperparameters)')"
   ],
   "id": "a3e7e1b210a366c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 16. Residual vs. Attention ‚Äì Model Performance Comparison\n",
    "Here we compare the residual and attention models side-by-side under both manual and tuned configurations. Validation accuracy and loss plots reveal that the attention model consistently outperforms the residual model in both scenarios.\n"
   ],
   "id": "cc7122b38d31041"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # ---------------- Comparative Plots ----------------\n",
    "# Compare residual vs attention models under manual training\n",
    "compare_models([residual_history, attention_history], ['Residual (Manual)', 'Attention (Manual)'], 'Manual Models')\n",
    "\n",
    "# Compare residual vs attention models after hyperparameter tuning\n",
    "compare_models([residual_history_tuned, attention_history_tuned], ['Residual (Tuned)', 'Attention (Tuned)'], 'Tuned Models')\n",
    "\n",
    "# Compare manual vs tuned for residual model\n",
    "compare_models([residual_history, residual_history_tuned], ['Residual (Manual)', 'Residual (Tuned)'], 'Residual: Manual vs Tuned')\n",
    "\n",
    "# Compare manual vs tuned for attention model\n",
    "compare_models([attention_history, attention_history_tuned], ['Attention (Manual)', 'Attention (Tuned)'], 'Attention: Manual vs Tuned')"
   ],
   "id": "cdfcebf4603bedad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
